{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding images and resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "\"\"\"def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\"\"\"\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    global first_flag\n",
    "    # Open and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    crop_margin_w = w // 15\n",
    "    crop_margin_h = h // 15\n",
    "\n",
    "    # Crop 1/20 from each side\n",
    "    image = image.crop((\n",
    "        crop_margin_w,                  # left\n",
    "        crop_margin_h,                  # top\n",
    "        w - crop_margin_w,              # right\n",
    "        h - crop_margin_h               # bottom\n",
    "    ))\n",
    "\n",
    "    # Resize to 1/2 of original dimensions\n",
    "    new_size = (w // 8, h // 8)\n",
    "    image = image.resize(new_size, Image.LANCZOS)\n",
    "    \n",
    "    if first_flag:\n",
    "        print(f\"new image size: {new_size}\")\n",
    "        first_flag = False\n",
    "        \n",
    "    # Convert to base64\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtlHRBAGV_Pn"
   },
   "source": [
    "Step 1: Analyze Normal Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UzVy3efBVbLz",
    "outputId": "50feb0ca-a95c-4757-82fc-df0092941f98"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import base64\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client = openai.OpenAI(api_key=\"..................................\") # Replace it with yours please!\n",
    "\n",
    "def describe_image_gpt(image_path):\n",
    "    image_analysis_prompt = (\n",
    "        f\"This is a normal {sub_class}. Analyze the image and describe the normal {sub_class} in detail, \"\n",
    "        \"including type, color, size (length, width), material, composition, quantity, \"\n",
    "        \"relative location.\"\n",
    "    )\n",
    "\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": image_analysis_prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_completion_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "ujT60Ti8Kd9p"
   },
   "outputs": [],
   "source": [
    "# Run step 1\n",
    "def step1(normal_image_paths):\n",
    "  normal_descriptions = []\n",
    "  for path in normal_image_paths:\n",
    "      display(Image.open(path))\n",
    "      desc = describe_image_gpt(path)\n",
    "      print(desc, \"\\n\" + \"=\"*80)\n",
    "      normal_descriptions.append(desc)\n",
    "  return normal_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBC5FuyXWXA_"
   },
   "source": [
    "Step 2: Summarizing the Normal Image Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpH6Jz3iYaTY",
    "outputId": "e45d4afc-65ec-4665-c52d-599325d68a26"
   },
   "outputs": [],
   "source": [
    "def step2(normal_descriptions):\n",
    "  # Build the prompt using collected descriptions\n",
    "  summary_prompt = (\n",
    "      f\"[ Normal {sub_class} Description 1 ]\\n{normal_descriptions[0]}\\n\"\n",
    "      f\"[ Normal {sub_class} Description 2 ]\\n{normal_descriptions[1]}\\n\"\n",
    "      f\"[ Normal {sub_class} Description 3 ]\\n{normal_descriptions[2]}\\n\"\n",
    "      \"Combine the three descriptions into one by extracting only the \\\"common\\\" features.\\n\"\n",
    "      \"Create a concise summary that reflects the shared characteristics while removing any \"\n",
    "      \"redundant or unique details.\"\n",
    "  )\n",
    "\n",
    "  # Send to GPT-4.1 for summarization\n",
    "  response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "      max_completion_tokens=500\n",
    "  )\n",
    "\n",
    "  summary_description = response.choices[0].message.content.strip()\n",
    "\n",
    "  # Output summary\n",
    "  print(f\"Summary of Normal {sub_class}\\n\")\n",
    "  print(summary_description)\n",
    "  return summary_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpzBf0aGXEtH"
   },
   "source": [
    "Step 3: Generating Main Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OR1m9Ik2Wp5h",
    "outputId": "4a6b8d15-0e83-4773-acc0-aa37e90dce86"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def step3(summary_description):\n",
    "  # Create question generation prompt from the summary\n",
    "  question_prompt = (\n",
    "      f\"[ Description of {sub_class} ]\\n{summary_description}\\n\\n\"\n",
    "      f\"Using the [ Description of {sub_class} ], create several\\n\"\n",
    "      f\"but essential, simple and important questions to determine whether the {sub_class} in the image is\\n\"\n",
    "      \"normal or abnormal. Ensure the questions are only based on visible characteristics, excluding\\n\"\n",
    "      \"any aspects that cannot be determined from the image. Also, simplify any difficult terms into\\n\"\n",
    "      \"easy-to-understand questions.\\n\"\n",
    "      \"(Q1) : ...\"\n",
    "  )\n",
    "\n",
    "  # Send to GPT-4.1 to generate main questions\n",
    "  response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": question_prompt}],\n",
    "      max_completion_tokens=500\n",
    "  )\n",
    "\n",
    "  # Extract questions\n",
    "  main_question_block = response.choices[0].message.content.strip()\n",
    "  print(\"Main Questions\\n\")\n",
    "  print(main_question_block)\n",
    "\n",
    "  print()\n",
    "\n",
    "  # split them into a list\n",
    "  main_questions = re.findall(r'\\(Q\\d+\\)\\s*:\\s*(.*)', main_question_block)\n",
    "  print(main_questions)\n",
    "  return main_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "muiZVYD-SYrL"
   },
   "outputs": [],
   "source": [
    "# Ask GPT-4.1 with question and image\n",
    "def evaluate_image_question(image_path, question):\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    prompt = (\n",
    "        f\"Question : {question}\\n\"\n",
    "        f\"At first, describe the {sub_class} and then answer the question. \\n\"\n",
    "        \"Your response must end with ‘- Result: Yes‘ or ‘- Result: No‘.\\n\"\n",
    "        \"Let’s think step by step.\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_completion_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "OZntT9K1FiE6"
   },
   "outputs": [],
   "source": [
    "def filter_questions(main_questions, normal_image_dir, max_images=10, rejection_threshold=0.2):\n",
    "    # Load image paths\n",
    "    image_paths = [os.path.join(normal_image_dir, f) for f in os.listdir(normal_image_dir)\n",
    "                  if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    image_paths = image_paths[:max_images]\n",
    "\n",
    "    # Evaluate each question and track rejection rate\n",
    "    filtered_questions = []\n",
    "\n",
    "    for question in main_questions:\n",
    "        no_count = 0\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            response_text = evaluate_image_question(image_path, question)\n",
    "            last_line = response_text.splitlines()[-1].lower()\n",
    "            if \"result: no\" in last_line:\n",
    "                no_count += 1\n",
    "            elif \"result: yes\" not in last_line:\n",
    "                print(\"Wrong answer, check model response: \" + response_text.strip())\n",
    "                exit()\n",
    "\n",
    "        rejection_ratio = no_count / len(image_paths)\n",
    "        print(f\"Question: {question[:60]}... → {rejection_ratio*100:.1f}% No\")\n",
    "\n",
    "        if rejection_ratio <= rejection_threshold:\n",
    "            filtered_questions.append(question)\n",
    "\n",
    "\n",
    "    print(\"\\n✔️ Final Kept Questions:\")\n",
    "    for q in filtered_questions:\n",
    "        print(\"- \", q)\n",
    "    return filtered_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8R_DsEqYObn"
   },
   "source": [
    "Step 4: Generating 5 Variations of Each Main Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynLJglBXXk5n",
    "outputId": "7b06af65-a473-4b4c-f197-b9784b19fa11"
   },
   "outputs": [],
   "source": [
    "def step4(main_questions):\n",
    "    # Store all variations\n",
    "    question_variations = {}\n",
    "\n",
    "    # Loop through each main question and generate 5 variations\n",
    "    for i, q in enumerate(main_questions):\n",
    "        variation_prompt = (\n",
    "            f\"Generate five variations of the following question while keeping the semantic meaning.\\n\"\n",
    "            f\"Input : {q}\\n\"\n",
    "            \"Output1:\\nOutput2:\\nOutput3:\\nOutput4:\\nOutput5:\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": variation_prompt}],\n",
    "            max_completion_tokens=500\n",
    "        )\n",
    "\n",
    "        # Parse and clean the outputs\n",
    "        outputs = re.findall(r'Output\\d+\\s*:\\s*(.*)', response.choices[0].message.content.strip())\n",
    "        question_variations[q] = outputs\n",
    "\n",
    "    # Show result\n",
    "    for main_q, variations in question_variations.items():\n",
    "        print(f\"\\n🔸 Original: {main_q}\")\n",
    "        for i, v in enumerate(variations, 1):\n",
    "            print(f\"  Var {i}: {v}\")\n",
    "    return question_variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy3j5NdFY76H"
   },
   "source": [
    "Step 5: Evaluating Test Images with the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "NxsQt1HdYmF3"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def step5(test_image_dir, question_variations, ground_truth, id, where_left=0, checkpoint_every=5):\n",
    "    global results, preds, gts, scores\n",
    "\n",
    "    good_paths = glob.glob(os.path.join(test_image_dir, \"good\", \"*.[jp][pn]g\"))\n",
    "    anomaly_paths = glob.glob(os.path.join(test_image_dir, \"logical_anomalies\", \"*.[jp][pn]g\"))\n",
    "\n",
    "    test_image_paths = good_paths + anomaly_paths\n",
    "    test_image_paths = shuffle(test_image_paths, random_state=42)\n",
    "\n",
    "    # Resume from checkpointed state or start fresh\n",
    "    if where_left == 0:\n",
    "        results = {}\n",
    "        preds = []\n",
    "        gts = []\n",
    "        scores = []\n",
    "    else:\n",
    "        try:\n",
    "            state = np.load(f\"logicqa_state_{id}.npz\")\n",
    "            preds = state[\"preds\"].tolist()\n",
    "            gts = state[\"gts\"].tolist()\n",
    "            scores = state[\"scores\"].tolist()\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Could not load preds/gts/scores:\", e)\n",
    "            preds, gts, scores = [], [], []\n",
    "\n",
    "        try:\n",
    "            with open(f\"logicqa_results_{id}.json\", \"r\") as f:\n",
    "                results = json.load(f)\n",
    "        except:\n",
    "            results = {}\n",
    "\n",
    "    # Slice image list to resume from where_left\n",
    "    test_image_paths = test_image_paths[where_left:]\n",
    "\n",
    "    for i, image_path in enumerate(test_image_paths, start=where_left):\n",
    "        print(f\"    🔄 Eval Progress: {i}/{where_left + len(test_image_paths)} - {os.path.basename(image_path)}\", end=\"\\r\", flush=True)\n",
    "        results[image_path] = {}\n",
    "\n",
    "        vote_results = []\n",
    "\n",
    "        for main_q, variations in question_variations.items():\n",
    "            all_qs = [main_q] + variations\n",
    "            no_votes = 0\n",
    "            yes_votes = 0\n",
    "            responses = []\n",
    "\n",
    "            for q in all_qs:\n",
    "                answer = evaluate_image_question(image_path, q)\n",
    "                last_line = answer.strip().splitlines()[-1].lower()\n",
    "                is_no = \"result: no\" in last_line\n",
    "                no_votes += is_no\n",
    "                yes_votes += not is_no\n",
    "                responses.append({\"question\": q, \"response\": answer})\n",
    "\n",
    "            majority_is_no = no_votes > yes_votes\n",
    "            vote_results.append(majority_is_no)\n",
    "\n",
    "            results[image_path][main_q] = {\n",
    "                \"responses\": responses,\n",
    "                \"majority_vote\": \"No\" if majority_is_no else \"Yes\"\n",
    "            }\n",
    "\n",
    "        final_pred = int(any(vote_results))\n",
    "        preds.append(final_pred)\n",
    "        gts.append(ground_truth.get(image_path, 0))\n",
    "\n",
    "        per_question_scores = [\n",
    "            sum(\"result: no\" in r[\"response\"].lower() for r in results[image_path][q][\"responses\"]) / len(results[image_path][q][\"responses\"])\n",
    "            for q in results[image_path] if results[image_path][q][\"responses\"]\n",
    "        ]\n",
    "        image_score = max(per_question_scores) if per_question_scores else 0\n",
    "        scores.append(image_score)\n",
    "\n",
    "        # ✅ Save checkpoint every N images\n",
    "        if (i + 1) % checkpoint_every == 0 or (i + 1 == where_left + len(test_image_paths)):\n",
    "            print(f\"\\n💾 Saving checkpoint at image {i+1}\")\n",
    "            with open(f\"logicqa_results_{id}.json\", \"w\") as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            np.savez(f\"logicqa_state_{id}.npz\", preds=np.array(preds), gts=np.array(gts), scores=np.array(scores))\n",
    "\n",
    "    # Final save\n",
    "    with open(f\"logicqa_results_{id}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    np.savez(f\"logicqa_state_{id}.npz\", preds=np.array(preds), gts=np.array(gts), scores=np.array(scores))\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    try:\n",
    "        auroc = roc_auc_score(gts, scores)\n",
    "    except:\n",
    "        auroc = \"AUROC: undefined (only one class present)\"\n",
    "    f1s = [f1_score(gts, [1 if s >= t else 0 for s in scores]) for t in np.linspace(0, 1, 100)]\n",
    "    f1_max = max(f1s)\n",
    "\n",
    "    print(\"\\n🔍 Evaluation Metrics:\")\n",
    "    print(f\"✔️ Accuracy: {acc:.3f}\")\n",
    "    print(f\"✔️ AUROC: {auroc if isinstance(auroc, str) else round(auroc, 3)}\")\n",
    "    print(f\"✔️ F1-Max: {f1_max:.3f}\")\n",
    "    return acc, auroc, f1_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xCJXYIFLFYl"
   },
   "source": [
    "A function to include all steps together and extracting image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "FEja3vL5LF0P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def extract_image_features(image_paths):\n",
    "    \"\"\"Extract features using a pretrained model (ResNet18)\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()  # remove classification head\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for path in image_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            x = transform(img).unsqueeze(0)  # add batch dim\n",
    "            feat = model(x).squeeze(0).numpy()\n",
    "            features.append(feat)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "def all_steps(normal_image_dir, test_image_dir, few_shot_mode, id):\n",
    "    global normal_descriptions, summary_description, main_questions, main_questions_filtered, question_variations\n",
    "    \n",
    "    all_normal_paths = [\n",
    "        os.path.join(normal_image_dir, f)\n",
    "        for f in os.listdir(normal_image_dir)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    if \"random\" in few_shot_mode:\n",
    "        three_normal_paths = random.sample(all_normal_paths, 3)\n",
    "\n",
    "    elif \"clustering_based\" in few_shot_mode:\n",
    "        features = extract_image_features(all_normal_paths)\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0).fit(features)\n",
    "        centers = kmeans.cluster_centers_\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        three_normal_paths = []\n",
    "        for k in range(3):\n",
    "            cluster_indices = np.where(labels == k)[0]\n",
    "            cluster_feats = features[cluster_indices]\n",
    "            dists = np.linalg.norm(cluster_feats - centers[k], axis=1)\n",
    "            closest_idx = cluster_indices[np.argmin(dists)]\n",
    "            three_normal_paths.append(all_normal_paths[closest_idx])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported few_shot_mode: {few_shot_mode}\")\n",
    "    \n",
    "\n",
    "\n",
    "    ground_truth = {}\n",
    "    for path in test_image_dir:\n",
    "        label = 0 if \"good\" in path else 1\n",
    "        ground_truth[path] = label\n",
    "\n",
    "    # Step-by-step LogicQA pipeline\n",
    "    normal_descriptions = step1(three_normal_paths)\n",
    "    summary_description = step2(normal_descriptions)\n",
    "    main_questions = step3(summary_description)\n",
    "    main_questions_filtered = filter_questions(main_questions, normal_image_dir, max_images=100, rejection_threshold=0.2)\n",
    "    question_variations = step4(main_questions_filtered)\n",
    "    acc, auroc, f1_max = step5(test_image_dir, question_variations, ground_truth, id)\n",
    "    \n",
    "    return acc, auroc, f1_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "sub_class = \"breakfast_box\"\n",
    "normal_image_dir = f\"C:\\\\Users\\\\sevza\\\\Desktop\\\\NLP\\proj\\\\mvtec_loco_anomaly_detection\\\\{sub_class}\\\\train\\\\good\\\\\"\n",
    "test_image_dir = f\"C:\\\\Users\\\\sevza\\\\Desktop\\\\NLP\\proj\\\\mvtec_loco_anomaly_detection\\\\{sub_class}\\\\test\\\\\"\n",
    "model = 'gpt-4.1'\n",
    "\n",
    "# To collect metrics: each a list of 10 values\n",
    "random_acc, random_auroc, random_f1 = [], [], []\n",
    "cluster_acc, cluster_auroc, cluster_f1 = [], [], []\n",
    "\n",
    "first_flag = True\n",
    "id = 0\n",
    "for i in range(3):\n",
    "    id = id + 1\n",
    "    print(f\"\\n🔁 Random Mode Run {i+1}\")\n",
    "    acc, auroc, f1 = all_steps(normal_image_dir, test_image_dir, few_shot_mode=\"random\", id=id)\n",
    "    random_acc.append(acc)\n",
    "    random_auroc.append(auroc)\n",
    "    random_f1.append(f1)\n",
    "\n",
    "    id = id + 1\n",
    "    print(f\"\\n🔁 Clustering-Based Mode Run {i+1}\")\n",
    "    acc, auroc, f1 = all_steps(normal_image_dir, test_image_dir, few_shot_mode=\"clustering_based\", id=id)\n",
    "    cluster_acc.append(acc)\n",
    "    cluster_auroc.append(auroc)\n",
    "    cluster_f1.append(f1)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "random_acc = np.array(random_acc)\n",
    "cluster_acc = np.array(cluster_acc)\n",
    "random_auroc = np.array(random_auroc)\n",
    "cluster_auroc = np.array(cluster_auroc)\n",
    "random_f1 = np.array(random_f1)\n",
    "cluster_f1 = np.array(cluster_f1)\n",
    "\n",
    "# Paired t-tests\n",
    "def compare(metric_name, rand, clust):\n",
    "    print(f\"\\n📊 {metric_name}\")\n",
    "    print(f\"  - Random     Mean ± Std: {rand.mean():.3f} ± {rand.std():.3f}\")\n",
    "    print(f\"  - Clustering Mean ± Std: {clust.mean():.3f} ± {clust.std():.3f}\")\n",
    "    t_stat, p_val = ttest_rel(clust, rand)\n",
    "    print(f\"  - Paired t-test: t = {t_stat:.3f}, p = {p_val:.4f} {'(significant)' if p_val < 0.05 else '(not significant)'}\")\n",
    "\n",
    "compare(\"Accuracy\", random_acc, cluster_acc)\n",
    "compare(\"AUROC\", random_auroc, cluster_auroc)\n",
    "compare(\"F1-max\", random_f1, cluster_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: Ignore the following cells and the accuracies shown in its output. This cell is designed to handle unexpected connection errors that occurred during execution and to avoid re-running the entire program. Additionally, note that the accuracy values reported in this cell is incorrect due to an error in the ground truth labels. The correct metrics, including accuracy, are calculated based on the saved result files in the final two cells of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Clustering-Based Mode Run 2\n",
      "\n",
      "🔸 Original: Is the box made of light-colored, disposable material with two main compartments (a smaller one on the left and a larger one on the right)?\n",
      "  Var 1: Is the box constructed from a light-toned, disposable material and does it have two primary compartments, with a smaller compartment on the left and a larger one on the right?\n",
      "  Var 2: Does the box consist of a light-colored, throwaway material and feature two main sections, with the left side being smaller and the right side larger?\n",
      "  Var 3: Is the box composed of disposable, light-colored material and split into two main compartments, the smaller on the left and the larger on the right?\n",
      "  Var 4: Is the box made out of a light-colored disposable material with two main sections, with a smaller one to the left and a bigger one to the right?\n",
      "  Var 5: Does the box use a light-colored, single-use material and have two primary compartments—a smaller on the left and a larger on the right?\n",
      "\n",
      "🔸 Original: Are there 2 or 3 small oranges (or clementines/mandarins) and 1 whole red or red-yellow apple together in the left compartment?\n",
      "  Var 1: Are there two or three small oranges (including clementines or mandarins) along with one whole red or red-yellow apple in the left compartment?\n",
      "  Var 2: Does the left compartment contain 2 or 3 small oranges (or clementines/mandarins) together with one whole red or red-yellow apple?\n",
      "  Var 3: Are there a total of 2 or 3 clementines, mandarins, or small oranges and one whole apple (red or red-yellow) all placed together in the left compartment?\n",
      "  Var 4: In the left compartment, are there 2 or 3 small oranges (clementines/mandarins) and a whole apple that is either red or red-yellow?\n",
      "  Var 5: Is there a group of 2 or 3 small oranges (or clementines/mandarins) along with one entire red or red-yellow apple in the left compartment?\n",
      "\n",
      "🔸 Original: Is the right compartment visually divided into two sections—one with a large pile of granola or oats (possibly with nuts/seeds), and the other with banana chips and whole almonds next to each other?\n",
      "  Var 1: Does the right compartment appear to be split into two areas, with one side containing a large mound of granola or oats (potentially including nuts or seeds), and the other featuring banana chips positioned beside whole almonds?\n",
      "  Var 2: Is the right-hand section visually separated into two distinct parts—one part filled with a substantial amount of granola or oats (which may have nuts/seeds), and the other containing both banana chips and whole almonds side by side?\n",
      "  Var 3: Can you see that the right compartment is divided into two portions: one section has a significant pile of granola or oats (possibly mixed with nuts or seeds), while the other section has banana chips placed next to whole almonds?\n",
      "  Var 4: Does it look like the right compartment is organized into two segments—one containing a large heap of granola or oats (possibly with some nuts or seeds mixed in), and the other housing banana chips and whole almonds adjacent to each other?\n",
      "  Var 5: Is the right-side compartment sectioned into two areas, with one area holding a big pile of granola or oats (maybe including nuts or seeds) and the other area displaying banana chips alongside whole almonds?\n",
      "\n",
      "🔸 Original: Does the overall arrangement match: fruit on the left, dry mix (granola/oats, banana chips, and almonds) on the right?\n",
      "  Var 1: Is the composition set up as described: fruit positioned on the left and the dry mix (granola/oats, banana chips, and almonds) on the right?\n",
      "  Var 2: Can you confirm if the items are arranged with fruit to the left and the dry mix (granola/oats, banana chips, and almonds) to the right?\n",
      "  Var 3: Is the layout such that the fruit is on the left side, and the dry mixture (including granola/oats, banana chips, and almonds) is on the right?\n",
      "  Var 4: Does the placement follow this order: fruit on the left side and dry mix (granola/oats, banana chips, and almonds) on the right side?\n",
      "  Var 5: Are the components organized so that the fruit appears on the left and the dry mix (granola/oats, banana chips, almonds) on the right?\n",
      "    🔄 Eval Progress: 4/100 - 044.png\n",
      "💾 Saving checkpoint at image 5\n",
      "    🔄 Eval Progress: 9/100 - 000.png\n",
      "💾 Saving checkpoint at image 10\n",
      "    🔄 Eval Progress: 14/100 - 040.png\n",
      "💾 Saving checkpoint at image 15\n",
      "    🔄 Eval Progress: 19/100 - 031.png\n",
      "💾 Saving checkpoint at image 20\n",
      "    🔄 Eval Progress: 24/100 - 019.png\n",
      "💾 Saving checkpoint at image 25\n",
      "    🔄 Eval Progress: 29/100 - 022.png\n",
      "💾 Saving checkpoint at image 30\n",
      "    🔄 Eval Progress: 34/100 - 043.png\n",
      "💾 Saving checkpoint at image 35\n",
      "    🔄 Eval Progress: 39/100 - 016.png\n",
      "💾 Saving checkpoint at image 40\n",
      "    🔄 Eval Progress: 44/100 - 027.png\n",
      "💾 Saving checkpoint at image 45\n",
      "    🔄 Eval Progress: 49/100 - 013.png\n",
      "💾 Saving checkpoint at image 50\n",
      "    🔄 Eval Progress: 54/100 - 008.png\n",
      "💾 Saving checkpoint at image 55\n",
      "    🔄 Eval Progress: 59/100 - 039.png\n",
      "💾 Saving checkpoint at image 60\n",
      "    🔄 Eval Progress: 64/100 - 000.png\n",
      "💾 Saving checkpoint at image 65\n",
      "    🔄 Eval Progress: 69/100 - 047.png\n",
      "💾 Saving checkpoint at image 70\n",
      "    🔄 Eval Progress: 74/100 - 048.png\n",
      "💾 Saving checkpoint at image 75\n",
      "    🔄 Eval Progress: 79/100 - 009.png\n",
      "💾 Saving checkpoint at image 80\n",
      "    🔄 Eval Progress: 84/100 - 001.png\n",
      "💾 Saving checkpoint at image 85\n",
      "    🔄 Eval Progress: 89/100 - 037.png\n",
      "💾 Saving checkpoint at image 90\n",
      "    🔄 Eval Progress: 94/100 - 020.png\n",
      "💾 Saving checkpoint at image 95\n",
      "    🔄 Eval Progress: 99/100 - 001.png\n",
      "💾 Saving checkpoint at image 100\n",
      "\n",
      "🔍 Evaluation Metrics:\n",
      "✔️ Accuracy: 0.470\n",
      "✔️ AUROC: AUROC: undefined (only one class present)\n",
      "✔️ F1-Max: 0.000\n",
      "\n",
      "📊 Accuracy\n",
      "  - Random     Mean ± Std: 0.265 ± 0.205\n",
      "  - Clustering Mean ± Std: 0.570 ± 0.100\n",
      "  - Paired t-test: t = 2.905, p = 0.2111 (not significant)\n",
      "\n",
      "📊 AUROC\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U41'), dtype('<U41')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Paired t-test: t = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_stat\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(significant)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp_val\u001b[38;5;250m \u001b[39m\u001b[38;5;241m<\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0.05\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(not significant)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m compare(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_acc, cluster_acc)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mcompare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAUROC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_auroc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_auroc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m compare(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-max\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_f1, cluster_f1)\n",
      "Cell \u001b[1;32mIn[122], line 22\u001b[0m, in \u001b[0;36mcompare\u001b[1;34m(metric_name, rand, clust)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare\u001b[39m(metric_name, rand, clust):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Random     Mean ± Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrand\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Clustering Mean ± Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclust\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclust\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     t_stat, p_val \u001b[38;5;241m=\u001b[39m ttest_rel(clust, rand)\n",
      "File \u001b[1;32mC:\\Python-3.11.4\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U41'), dtype('<U41')) -> None"
     ]
    }
   ],
   "source": [
    "# IGNORE THIS CELL: this cell is designed to handle a connection error and avoid re-running already executed parts.\n",
    "\n",
    "i = 1\n",
    "print(f\"\\n🔁 Clustering-Based Mode Run {i+1}\")\n",
    "question_variations = step4(main_questions_filtered)\n",
    "acc, auroc, f1 = step5(test_image_dir, question_variations, ground_truth, id)\n",
    "    \n",
    "#acc, auroc, f1 = all_steps(normal_image_dir, test_image_dir, few_shot_mode=\"clustering_based\", id=id)\n",
    "cluster_acc.append(acc)\n",
    "cluster_auroc.append(auroc)\n",
    "cluster_f1.append(f1)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "random_acc = np.array(random_acc)\n",
    "cluster_acc = np.array(cluster_acc)\n",
    "random_auroc = np.array(random_auroc)\n",
    "cluster_auroc = np.array(cluster_auroc)\n",
    "random_f1 = np.array(random_f1)\n",
    "cluster_f1 = np.array(cluster_f1)\n",
    "\n",
    "# Paired t-tests\n",
    "def compare(metric_name, rand, clust):\n",
    "    print(f\"\\n📊 {metric_name}\")\n",
    "    print(f\"  - Random     Mean ± Std: {rand.mean():.3f} ± {rand.std():.3f}\")\n",
    "    print(f\"  - Clustering Mean ± Std: {clust.mean():.3f} ± {clust.std():.3f}\")\n",
    "    t_stat, p_val = ttest_rel(clust, rand)\n",
    "    print(f\"  - Paired t-test: t = {t_stat:.3f}, p = {p_val:.4f} {'(significant)' if p_val < 0.05 else '(not significant)'}\")\n",
    "\n",
    "compare(\"Accuracy\", random_acc, cluster_acc)\n",
    "compare(\"AUROC\", random_auroc, cluster_auroc)\n",
    "compare(\"F1-max\", random_f1, cluster_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Random Run 1 — logicqa_results_1.json\n",
      "   ✔️ Accuracy: 0.870\n",
      "   📈 AUROC: 0.8907999999999999\n",
      "   ⭐ F1-max: 0.874\n",
      "\n",
      "📊 Clustering-based Run 2 — logicqa_results_2.json\n",
      "   ✔️ Accuracy: 0.830\n",
      "   📈 AUROC: 0.8300000000000001\n",
      "   ⭐ F1-max: 0.795\n",
      "\n",
      "📊 Random Run 3 — logicqa_results_3.json\n",
      "   ✔️ Accuracy: 0.540\n",
      "   📈 AUROC: 0.7836000000000001\n",
      "   ⭐ F1-max: 0.739\n",
      "\n",
      "📊 Clustering-based Run 4 — logicqa_results_4.json\n",
      "   ✔️ Accuracy: 0.890\n",
      "   📈 AUROC: 0.904\n",
      "   ⭐ F1-max: 0.893\n",
      "\n",
      "📈 Aggregated Results:\n",
      "\n",
      "🌀 Random Mode:\n",
      "   ✔️ Accuracy: 0.705 ± 0.165\n",
      "   📈 AUROC: 0.837 ± 0.054\n",
      "   ⭐ F1-max: 0.806 ± 0.067\n",
      "\n",
      "🌟 Clustering-Based Mode:\n",
      "   ✔️ Accuracy: 0.860 ± 0.030\n",
      "   📈 AUROC: 0.867 ± 0.037\n",
      "   ⭐ F1-max: 0.844 ± 0.049\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "def is_normal(filename):\n",
    "    return \"good\" in filename.lower()\n",
    "\n",
    "def majority_votes_to_final_label(votes):\n",
    "    return int(any(\"no\" in v.lower() for v in votes))\n",
    "\n",
    "# Containers for statistics\n",
    "random_acc, random_auroc, random_f1 = [], [], []\n",
    "cluster_acc, cluster_auroc, cluster_f1 = [], [], []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    file_path = f\"logicqa_results_{i}.json\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    scores = []  # anomaly scores = proportion of \"no\" answers\n",
    "\n",
    "    for image_path, question_dict in results.items():\n",
    "        majority_votes = [v[\"majority_vote\"].lower() for v in question_dict.values()]\n",
    "\n",
    "        pred = majority_votes_to_final_label(majority_votes)\n",
    "        gt = 0 if is_normal(image_path) else 1\n",
    "        score = sum(\"no\" in v for v in majority_votes) / len(majority_votes)\n",
    "\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "        scores.append(score)\n",
    "\n",
    "    acc = sum(p == g for p, g in zip(preds, gts)) / len(preds) if preds else 0\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(gts, scores)\n",
    "    except:\n",
    "        auroc = np.nan  # If AUROC is not computable\n",
    "\n",
    "    f1s = [f1_score(gts, [1 if s >= t else 0 for s in scores]) for t in np.linspace(0, 1, 100)]\n",
    "    f1_max = max(f1s)\n",
    "\n",
    "    # Store in appropriate list\n",
    "    if i in [1, 3]:  # Random runs\n",
    "        random_acc.append(acc)\n",
    "        random_auroc.append(auroc)\n",
    "        random_f1.append(f1_max)\n",
    "        run_type = \"Random\"\n",
    "    else:            # Clustering-based runs\n",
    "        cluster_acc.append(acc)\n",
    "        cluster_auroc.append(auroc)\n",
    "        cluster_f1.append(f1_max)\n",
    "        run_type = \"Clustering-based\"\n",
    "\n",
    "    print(f\"\\n📊 {run_type} Run {i} — logicqa_results_{i}.json\")\n",
    "    print(f\"   ✔️ Accuracy: {acc:.3f}\")\n",
    "    print(f\"   📈 AUROC: {auroc if not np.isnan(auroc) else 'undefined'}\")\n",
    "    print(f\"   ⭐ F1-max: {f1_max:.3f}\")\n",
    "\n",
    "# Convert to NumPy arrays for stats\n",
    "random_acc = np.array(random_acc)\n",
    "random_auroc = np.array(random_auroc)\n",
    "random_f1 = np.array(random_f1)\n",
    "\n",
    "cluster_acc = np.array(cluster_acc)\n",
    "cluster_auroc = np.array(cluster_auroc)\n",
    "cluster_f1 = np.array(cluster_f1)\n",
    "\n",
    "# Print Mean and Std\n",
    "print(\"\\n📈 Aggregated Results:\")\n",
    "\n",
    "print(\"\\n🌀 Random Mode:\")\n",
    "print(f\"   ✔️ Accuracy: {random_acc.mean():.3f} ± {random_acc.std():.3f}\")\n",
    "print(f\"   📈 AUROC: {np.nanmean(random_auroc):.3f} ± {np.nanstd(random_auroc):.3f}\")\n",
    "print(f\"   ⭐ F1-max: {random_f1.mean():.3f} ± {random_f1.std():.3f}\")\n",
    "\n",
    "print(\"\\n🌟 Clustering-Based Mode:\")\n",
    "print(f\"   ✔️ Accuracy: {cluster_acc.mean():.3f} ± {cluster_acc.std():.3f}\")\n",
    "print(f\"   📈 AUROC: {np.nanmean(cluster_auroc):.3f} ± {np.nanstd(cluster_auroc):.3f}\")\n",
    "print(f\"   ⭐ F1-max: {cluster_f1.mean():.3f} ± {cluster_f1.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Example False Positive (normal → predicted abnormal):\n",
      "   📷 Image: C:\\Users\\sevza\\Desktop\\NLP\\proj\\mvtec_loco_anomaly_detection\\breakfast_box\\test\\good\\042.png\n",
      "   ❌ Questions answered 'No':\n",
      "     - Is the right compartment visually divided into two sections—one with a large pile of granola or oats (possibly with nuts/seeds), and the other with banana chips and whole almonds next to each other?\n",
      "\n",
      "🔍 Example False Negative (abnormal → predicted normal):\n",
      "   📷 Image: C:\\Users\\sevza\\Desktop\\NLP\\proj\\mvtec_loco_anomaly_detection\\breakfast_box\\test\\logical_anomalies\\046.png\n",
      "   📋 All Questions and Answers:\n",
      "     - Q: Is the box made of light-colored, disposable material with two main compartments (a smaller one on the left and a larger one on the right)?\n",
      "     - Q: Are there 2 or 3 small oranges (or clementines/mandarins) and 1 whole red or red-yellow apple together in the left compartment?\n",
      "     - Q: Is the right compartment visually divided into two sections—one with a large pile of granola or oats (possibly with nuts/seeds), and the other with banana chips and whole almonds next to each other?\n",
      "     - Q: Does the overall arrangement match: fruit on the left, dry mix (granola/oats, banana chips, and almonds) on the right?\n"
     ]
    }
   ],
   "source": [
    "# Load the last result file (clustering-based run 2)\n",
    "with open(\"logicqa_results_4.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "def get_misclassified_samples(results, find_normal=True):\n",
    "    for image_path, question_dict in results.items():\n",
    "        majority_votes = [v[\"majority_vote\"].lower() for v in question_dict.values()]\n",
    "        pred = majority_votes_to_final_label(majority_votes)\n",
    "        gt = 0 if is_normal(image_path) else 1\n",
    "\n",
    "        if (find_normal and is_normal(image_path) and pred != gt) or (not find_normal and not is_normal(image_path) and pred != gt):\n",
    "            return image_path, gt, pred, question_dict\n",
    "    return None, None, None, {}\n",
    "\n",
    "# Find one false positive (normal image → wrongly predicted as abnormal)\n",
    "fp_path, fp_gt, fp_pred, fp_question_dict = get_misclassified_samples(results, find_normal=True)\n",
    "\n",
    "# Find one false negative (abnormal image → wrongly predicted as normal)\n",
    "fn_path, fn_gt, fn_pred, fn_question_dict = get_misclassified_samples(results, find_normal=False)\n",
    "\n",
    "print(\"\\n🔍 Example False Positive (normal → predicted abnormal):\")\n",
    "if fp_path:\n",
    "    print(f\"   📷 Image: {fp_path}\")\n",
    "    print(\"   ❌ Questions answered 'No':\")\n",
    "    for q, v in fp_question_dict.items():\n",
    "        if \"no\" in v[\"majority_vote\"].lower():\n",
    "            print(f\"     - {q}\")\n",
    "else:\n",
    "    print(\"   ✅ No false positives found.\")\n",
    "\n",
    "print(\"\\n🔍 Example False Negative (abnormal → predicted normal):\")\n",
    "if fn_path:\n",
    "    print(f\"   📷 Image: {fn_path}\")\n",
    "    print(\"   📋 All Questions and Answers:\")\n",
    "    for q, v in fn_question_dict.items():\n",
    "        print(f\"     - Q: {q}\")\n",
    "        \"\"\"for idx, resp in enumerate(v[\"responses\"]):\n",
    "            print(f\"         Variant {idx}: {resp['response'].strip()}\")\"\"\"\n",
    "else:\n",
    "    print(\"   ✅ No false negatives found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
